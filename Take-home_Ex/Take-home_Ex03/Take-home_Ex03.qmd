---
title: "Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods"
date: "9 March 2023"
date-modified: "`r Sys.Date()`"
format: html
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
---

# 1 Setting the scene

Several things influence housing costs. Some of them have a worldwide scope, such the overall health of a nation's economy or the level of inflation. Some may focus more on the properties themselves. You can further divide these characteristics into structural and geographic ones.

Geographical Weighted Models were introduced for enhancing predictive model for housing resale prices as traditional price predictive models failed to take account the spatial correlation and heterogeneity in geographical aspects which caused inaccuracy and bias.

## 1.1 Loading the data

```{r}
pacman::p_load('sf', 'tidyverse', 'tmap', 'spdep', 'onemapsgapi', 'units', 'matrixStats', 'readxl', 'jsonlite', 'olsrr', 'corrplot', 'ggpubr', 'GWmodel',
'devtools', 'kableExtra', 'plotly', 'ggthemes', 'ranger', 'Metrics')
```

## 1.2 Data set used

```{r}
datasets <- data.frame(
  Type=c("Aspatial",
         "Geospatial",
         
         "Geospatial - Extracted",
         "Geospatial - Extracted",
         "Geospatial - Extracted",
         "Geospatial - Extracted",
         "Geospatial - Extracted",
         "Geospatial - Extracted",
         "Geospatial - Extracted",
         
         "Geospatial - Selfsourced",
         "Geospatial - Selfsourced",
         "Geospatial - Selfsourced",
         "Geospatial - Selfsourced",
         "Geospatial - Selfsourced"),
  
  Name=c("Resale Flat Prices",
         "Master Plan 2019 Subzone Boundary (Web)",
         
         "Childcare Services",
         "Community Clubs",
         "Eldercare Services",
         "Hawker Centres",
         "Kindergartens",
         "Parks",
         "Libraries",
         
         
         "Bus Stop Locations Aug 2021",
         "MRT & LRT Locations Aug 2021",
         "Supermarkets",
         "Shopping Mall SVY21 Coordinates", 
         "Primary School"),
  
  Format=c(".csv", 
           ".shp",
           
           ".shp", 
           ".shp", 
           ".shp", 
           ".shp",
           ".shp", 
           ".shp",
           ".shp",
           
           ".shp",
           ".kml",
           ".shp",
           ".shp",
           ".csv"),
  
  Source=c("[data.gov.sg](https://data.gov.sg/dataset/resale-flat-prices)",
           "[data.gov.sg](https://data.gov.sg/dataset/master-plan-2014-subzone-boundary-web)",
           
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           
           "[datamall.lta](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=bus%20stop)",
           "[data.gov](https://data.gov.sg/dataset/lta-mrt-station-exit)",
           "[Onemap.gov](https://www.onemap.gov.sg/main/v2/essentialamenities)",
           "[Valery Lim's Github](https://github.com/ValaryLim/Mall-Coordinates-Web-Scraper/blob/master/mall_coordinates_updated.csv)",
           "[data.gov](https://data.gov.sg/dataset/school-directory-and-information)")
  )

library(knitr)
library(kableExtra)
kable(datasets, caption="Datasets Used") %>%
  kable_material("hover", latex_options = "scale_down")
```

# 2 Data Preparation

## 2.1 Aspatial data

Loading Resale Data

::: panel-tabset
# Loading Data

```{r}
resale <- read_csv("data/aspatial/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv")
```

# Viewing Data

```{r}
glimpse(resale)
```
:::

On a personal note, I am currently staying in a 3 room flat at Bedok, and my family is planning to move soon. Hence, I have decided to scope it down to 3 room flats so that I can roughly gauge how much my family can resell the house! It will be interesting to look at how various geographical factors affect resale prices :-)

As such , we will be looking at 3 room flats, within January of 2021 to February of 2023, which will be separated again later on. This is done by using filter() on flat_type and month.

```{r}
resale_sub <-  resale %>%
  filter(flat_type == "3 ROOM",
         month >= "2021-01" & month <= "2023-02")
```

To double confirm that we extracted what we really want, we will be using unique() to view what we extracted. As seen below, we have extracted the correct range for the dates, and the flat_type of 3 Room.

::: panel-tabset
# Checking range for month

```{r}
unique(resale_sub$month)

```

# Checking flat type

```{r}
unique(resale_sub$flat_type)
```

# Viewing resale_sub

```{r}
glimpse(resale_sub)

```
:::

### 2.1.2 Aspatial Wrangling

As we can see from the resale_sub, the variable remaining lease and lease commence date essentially provides the same information. We will only be including remaining lease, as it directly tells us the remaining year. Most importantly, this will help us to ensure that the variables are not perfectly collinear during our regression later!

```{r}
resale_sub <- resale_sub %>%
  select(-lease_commence_date)
```

Additionally, after looking at the resale_sub variables, we notice that remaining lease is \<chr\>. We would want it to be numeric, as it will be beneficial for us later on. We will have to first split the months and years in the variable remaining_lease.

::: panel-tabset
# Code Chunk

```{r}
resale_sub <- resale_sub %>%
mutate(remaining_lease_yr = as.integer(str_sub(remaining_lease, 0, 2))) %>%
  mutate(remaining_lease_mth = as.integer(str_sub(remaining_lease, 9, 11)))

```

# Glimpse

```{r}
glimpse(resale_sub)
```
:::

From above, we notice that there are NA values for remaining lease month. We will replace it with 0 by identifying it with is.na()

```{r}
resale_sub$remaining_lease_mth[is.na(resale_sub$remaining_lease_mth)] <- 0
```

Our main goal here is to convert remaining lease months into years, and add them together under remaining_lease_year. we will first divide the remaining lease month variable by 12.

```{r}
resale_sub$remaining_lease_mth <- resale_sub$remaining_lease_mth/12
```

Then, we will combine them together under the new variable remaining lease year.

```{r}
resale_sub <- resale_sub %>%
mutate(resale_sub, remaining_lease_year = rowSums(resale_sub[, c("remaining_lease_yr", "remaining_lease_mth")]))
```

By referencing to our favorite senior, [Megan](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/?panelset=self-sourced&panelset6=glimpse%28%29#data-wrangling-aspatial-data), she advised that we replace "SAINT" with "ST", as onemap spells it that way in the variable street_name, which will be a bit confusing. We can change that by using the code chunk below

```{r}
resale_sub$street_name <- gsub("ST\\.", "SAINT", resale_sub$street_name)
```

Similarly, by referencing Megan's work, we notice that there are no coordinates provided in the resale data. we will have to use a geocoding function as shown below.

The steps are as follows:

-   Combine the block and street name into an address

-   Pass the address as the searchVal in our query

-   Send the query to OneMapSG search *Note: Since we don't need all the address details, we can set `getAddrDetails` as 'N'*

-   Convert response (JSON object) to text

-   Saving the response in text form as a data frame

-   retaining the latitude and longitude for our output

```{r}
library(httr)
library(rjson)
geocode <- function(block, streetname) {
  base_url <- "https://developers.onemap.sg/commonapi/search"
  address <- paste(block, streetname, sep = " ")
  query <- list("searchVal" = address, 
                "returnGeom" = "Y",
                "getAddrDetails" = "N",
                "pageNum" = "1")
  
  res <- GET(base_url, query = query)
  restext<-content(res, as="text")
  
  output <- fromJSON(restext)  %>% 
    as.data.frame %>%
    select(results.LATITUDE, results.LONGITUDE)

  return(output)
}
```

After creating the function, we will have to make sure that the iterations are able to run through the whole resale data to ensure that every entry gets a coordinate. As such, we will be creating a loop

```{r, eval = FALSE}
resale_sub$LATITUDE <- 0
resale_sub$LONGITUDE <- 0

for (i in 1:nrow(resale_sub)){
  temp_output <- geocode(resale_sub[i, 4], resale_sub[i, 5])
  
  resale_sub$LATITUDE[i] <- temp_output$results.LATITUDE
  resale_sub$LONGITUDE[i] <- temp_output$results.LONGITUDE
}
```

Let us save it into a RDS object

```{r, eval = FALSE}
saveRDS(resale_sub, file="resale_sub", compress=FALSE)
```

We will then assign the RDS object to resale_sub

```{r}
resale_sub <- readRDS("resale_sub")
```

### 2.1.3 Creating ordinal arrangement for floor level

We will create an ordinal column for the floor level for the purpose of our analysis later on! We will first use unique() to identify all of the values from storey_range

```{r}
unique(resale_sub$storey_range)
```

We will then write it down into a vector, and assign it to levels. After that, we will assign story_ordinal by using seq_along() and changing story_ordinal into numeric using as.numeric().

```{r}
levels <- c("01 TO 03", "04 TO 06", "07 TO 09", "10 TO 12", "13 TO 15", "16 TO 18", "19 TO 21", "22 TO 24", "25 TO 27", "28 TO 30", "31 TO 33", "34 TO 36", "37 TO 39", "40 TO 42", "43 TO 45", "46 TO 48", "49 TO 51") 

story_ordinal <- seq_along(levels)

resale_sub$Story_Ordinal <- story_ordinal[match(resale_sub$storey_range, levels)]  

levels(resale_sub$Story_Ordinal) <- levels

resale_sub <- resale_sub |> 
  mutate(Story_Ordinal=as.numeric(Story_Ordinal))
```

### 2.1.4 Mutating remaining columns

We noticed that month is still \<chr\>, which is able to be changed by as.Date. By referencing to this [website,](https://www.statology.org/convert-strings-to-dates-in-r/) %Y denotes the year with century, %m for month, and %d for day.

```{r}
resale_sub <- resale_sub %>% 
  mutate(month = as.Date(paste(month, "-01"), format ="%Y-%m-%d"))
```

### 2.1.5 Checking for duplicates

Finally, let us check for duplicates. We are good to go!

```{r}
sum(is.na(resale_sub$LATITUDE))
sum(is.na(resale_sub$LONGITUDE))
```

## Geospatial data

As mentioned above, we will be extracting geospatial data from Onemap. the codes used below will require the onemapsgapi package, which is already loaded in 1.1. I will break down the steps to extract data from Onemap as simple as possible!

-   First, create an account for Onemap

-   Fill up your details in this [link](https://developers.onemap.sg/confirm_account/) (with the details sent to your email)

-   Ready to go after following the code chunks below!

Do note that this is purely for reference, you will have to type in your own email and password to load your token!

Type this command into the console, with your own email and password

```{r, eval = FALSE}
run_token <- get_token("email", "password")
```

Next, since I love food, I will be giving an example to extract hawker data from Onemap, using this code chunk.

```{r, eval = FALSE}
themes <- search_themes(run_token, "hawkercentre")
```

We will use the following function to extract data from Onemap. Lets assign it into hawker.

```{r, eval = FALSE}
hawkercentre <- get_theme(run_token, "hawkercentre")
```

And of course, our favorite part, ensuring that our spatial data have the correct ESPG CRS, which is 3414. We will also need to transform it into sf using st_as_sf(). Since we will be dealing with various extracted data, we do not want to keep extracting it each time we run or render the page. Hence, we will be using st_write() to write the sf object to file or database!

```{r, eval = FALSE}
hawkercentre_sf <- st_as_sf(hawkercentre, coords = c("Lng", "Lat"), crs = 4326) %>%
st_transform(crs = 3414)

st_write(obj = hawkercentre_sf,
         dsn = "data/geospatial/extracted",
         layer = "hawkercentre",
         driver = "ESRI Shapefile")
```

We can just repeat the steps above for the following variables. For the scope of this assignment, we will be extracting the following:

-   childcare

-   community clubs

-   elder care

-   kindergartens

-   libraries

-   parks

I have already extracted the data beforehand, so let us proceed!

(Note that we have already extracted hawker centers!)

For reference, i will be putting the code below to extract the data from onemap

::: panel-tabset
# Childcare

```{r, eval = FALSE}
childcare <- get_theme(token,"childcare")
childcare_sf <- st_as_sf(childcare, coords = c("Lng", "Lat"), crs = 4326) %>%
st_transform(crs = 3414)

st_write(obj = childcare_sf,
         dsn = "data/geospatial/extracted",
         layer = "childcare",
         driver = "ESRI Shapefile")
```

# Community clubs

```{r, eval = FALSE}
communityclubs<- get_theme(token,"communityclubs")
communityclubs_sf <- st_as_sf(childcare, coords = c("Lng", "Lat"), crs = 4326) %>%
st_transform(crs = 3414)

st_write(obj = communityclubs_sf,
         dsn = "data/geospatial/extracted",
         layer = "communityclubs",
         driver = "ESRI Shapefile")

```

# Elder care

```{r, eval = FALSE}

eldercare <- get_theme(token,"eldercare")
eldercare_sf <- st_as_sf(eldercare, coords=c("Lng", "Lat"), crs = 4326) %>%
st_transform(crs = 3414)

st_write(obj = eldercare_sf,
         dsn = "data/geospatial/extracted",
         layer = "eldercare",
         driver = "ESRI Shapefile")
```

# Kindergartens

```{r, eval = FALSE}
kindergartens <- get_theme(token,"kindergartens")
kindergartens_sf <- st_as_sf(kindergartens, coords=c("Lng", "Lat"), crs= 4326) %>%
st_transform(crs = 3414)

st_write(obj = kindergartens_sf,
         dsn = "data/geospatial/extracted",
         layer = "kindergartens",
         driver = "ESRI Shapefile")
```

# Libraries

```{r, eval = FALSE}
library <- get_theme(token,"libraries")
library_sf <- st_as_sf(library, coords=c("Lng", "Lat"), crs = 4326) %>%
st_transform(crs = 3414)

st_write(obj = library_sf,
         dsn = "data/geospatial/extracted",
         layer = "libraries",
         driver = "ESRI Shapefile")
```

# Parks

```{r, eval = FALSE}
parks <- get_theme(token,"nationalparks")
parks_sf <- st_as_sf(parks, coords=c("Lng", "Lat"), crs = 4326) %>%
st_transform(crs = 3414)

st_write(obj = parks_sf,
         dsn = "data/geospatial/extracted",
         layer = "parks",
         driver = "ESRI Shapefile")
```
:::

### 2.2.1 Reading Geospatial Data

::: panel-tabset
# Onemap (Extracted)

```{r}
hawkercentre_sf <- st_read(dsn = "data/geospatial/extracted", layer = "hawkercentre")
```

```{r}
eldercare_sf <- st_read(dsn = "data/geospatial/extracted", layer = "eldercare")
```

```{r}
childcare_sf <- st_read(dsn = "data/geospatial/extracted", layer = "childcare")
```

```{r}
communityclubs_sf <- st_read(dsn = "data/geospatial/extracted", layer = "communityclubs")
```

```{r}
kindergartens_sf <- st_read(dsn = "data/geospatial/extracted", layer = "kindergartens")
```

```{r}
library_sf <- st_read(dsn = "data/geospatial/extracted", layer = "libraries")
```

```{r}
parks_sf <- st_read(dsn = "data/geospatial/extracted", layer = "nationalparks")
```

# Basemap

```{r}
mpsz_sf <- st_read(dsn = "data/geospatial/map", layer = "MP14_SUBZONE_WEB_PL")
```

# Self-Sourced

```{r}
busstop_sf <- st_read(dsn = "data/geospatial/sourced", layer = "BusStop")
```

```{r}
goodprisch_sf <- st_read(dsn = "data/geospatial/sourced", layer = "goodprimarysch")
```

```{r}
mrt_sf <- st_read(dsn = "data/geospatial/sourced", layer = "mrt")
```

```{r}
prisch_sf <- st_read(dsn = "data/geospatial/sourced", layer = "primarysch")
```

```{r}
shoppingmall_sf <- st_read(dsn = "data/geospatial/sourced", layer = "shoppingmall")
```

```{r}
supermarket_sf <- st_read(dsn = "data/geospatial/sourced", layer = "SUPERMARKETS")
```
:::

### 2.2.2 Converting Self-sourced data

As we all know, we have yet to change our out-sourced data to the correct crs, which is 3414. Let us change it below using st_transform()! Do note that we do not need to do this all the time, especially when the data is already in the correct CRS. But in this case, all of the data are not.

```{r}
busstop_sf <- busstop_sf %>%
  st_transform(3414)
```

```{r}
goodprisch_sf <- goodprisch_sf %>%
  st_transform(3414)
```

```{r}
mrt_sf <- mrt_sf %>%
  st_transform(3414)
```

```{r}
prisch_sf <- prisch_sf %>%
  st_transform(3414)
```

```{r}
shoppingmall_sf <- shoppingmall_sf %>%
  st_transform(3414)
```

```{r}
supermarket_sf <- supermarket_sf %>%
  st_transform(3414)
```

### 2.2.3 Checking for invalid geometries

We will be checking invalid geometries using the length(), which() and st_is_valid() functions.

::: panel-tabset
## Onemap (Extracted)

```{r}
length(which(st_is_valid(hawkercentre_sf) == FALSE))
```

```{r}
length(which(st_is_valid(communityclubs_sf) == FALSE))
```

```{r}
length(which(st_is_valid(eldercare_sf) == FALSE))
```

```{r}
length(which(st_is_valid(childcare_sf) == FALSE))
```

```{r}
length(which(st_is_valid(kindergartens_sf) == FALSE))
```

```{r}
length(which(st_is_valid(library_sf) == FALSE))
```

```{r}
length(which(st_is_valid(parks_sf) == FALSE))
```

## Basemap

```{r}
length(which(st_is_valid(mpsz_sf) == FALSE))
```

## Self-Sourced

```{r}
length(which(st_is_valid(busstop_sf) == FALSE))
```

```{r}
length(which(st_is_valid(goodprisch_sf) == FALSE))
```

```{r}
length(which(st_is_valid(mrt_sf) == FALSE))
```

```{r}
length(which(st_is_valid(prisch_sf) == FALSE))
```

```{r}
length(which(st_is_valid(shoppingmall_sf) == FALSE))
```

```{r}
length(which(st_is_valid(supermarket_sf) == FALSE))
```
:::

Alright, that aside, we notice that the basemap have a few invalid geometries. This can be corrected using st_make_valid()

```{r}
mpsz_sf <- st_make_valid(mpsz_sf)
```

### 2.3.4 Selecting name column

As we only need the name and geometry column, we will use the select() function.

::: panel-tabset
## Onemap (Extracted)

```{r}
hawkercentre_sf <- hawkercentre_sf %>%
  select(1)
```

```{r}
communityclubs_sf <- communityclubs_sf %>%
  select(1)
```

```{r}
eldercare_sf <- eldercare_sf %>%
  select(1)
```

```{r}
childcare_sf <- childcare_sf %>%
  select(1)
```

```{r}
kindergartens_sf <- kindergartens_sf %>%
  select(1)
```

```{r}
library_sf <- library_sf %>%
  select(1)
```

```{r}
parks_sf <- parks_sf %>%
  select(1)
```

## Self-sourced

```{r}
busstop_sf <- busstop_sf %>%
  select(1)
```

```{r}
goodprisch_sf <- goodprisch_sf %>%
  select(1)
```

```{r}
mrt_sf <- mrt_sf %>%
  select(1)
```

```{r}
prisch_sf <- prisch_sf %>%
  select(1)
```

```{r}
shoppingmall_sf <- shoppingmall_sf %>%
  select(1)
```

```{r}
supermarket_sf <- supermarket_sf %>%
  select(1)
```
:::

### 2.3.5 Checking for missing values

What is more scary than missing geometries? Missing values, of course! That is a huge no no. Let us check missing values with is.na().

::: panel-tabset
## Onemap (Extracted)

```{r}
hawkercentre_sf[rowSums(is.na(hawkercentre_sf))!=0,]
```

```{r}
communityclubs_sf[rowSums(is.na(communityclubs_sf))!=0,]
```

```{r}
eldercare_sf[rowSums(is.na(eldercare_sf))!=0,]
```

```{r}
childcare_sf[rowSums(is.na(childcare_sf))!=0,]
```

```{r}
kindergartens_sf[rowSums(is.na(kindergartens_sf))!=0,]
```

```{r}
library_sf[rowSums(is.na(library_sf))!=0,]
```

```{r}
parks_sf[rowSums(is.na(parks_sf))!=0,]
```

## Basemap

```{r}
mpsz_sf[rowSums(is.na(mpsz_sf))!=0,]
```

## Self-Sourced

```{r}
busstop_sf[rowSums(is.na(busstop_sf))!=0,]
```

```{r}
goodprisch_sf[rowSums(is.na(goodprisch_sf))!=0,]
```

```{r}
mrt_sf[rowSums(is.na(mrt_sf))!=0,]
```

```{r}
prisch_sf[rowSums(is.na(prisch_sf))!=0,]
```

```{r}
shoppingmall_sf[rowSums(is.na(shoppingmall_sf))!=0,]
```

```{r}
supermarket_sf[rowSums(is.na(supermarket_sf))!=0,]
```
:::

# 3 Exploratory Data Analysis

For this section, we will be visualising data with tmap! For colour coding purposes,

-   Blue denotes public space and transport

-   Red denotes education

-   Green denotes park

-   Purple denotes elder care :-)

::: panel-tabset
## Hawker centers

```{r}
tmap_mode("plot")
tm_shape(mpsz_sf) +
  tm_polygons(alpha = 0.3) +
tm_shape(hawkercentre_sf) +
  tm_dots(col = "blue", size = 0.04)
```

## Community clubs

```{r}
tmap_mode("plot")
tm_shape(mpsz_sf) +
  tm_polygons(alpha = 0.3) +
tm_shape(communityclubs_sf) +
  tm_dots(col = "blue", size = 0.04)
```

## Elderly care

```{r}
tmap_mode("plot")
tm_shape(mpsz_sf) +
  tm_polygons(alpha = 0.3) +
tm_shape(eldercare_sf) +
  tm_dots(col = "purple", size = 0.04)
```

## Childcare

```{r}
tmap_mode("plot")
tm_shape(mpsz_sf) +
  tm_polygons(alpha = 0.3) +
tm_shape(childcare_sf) +
  tm_dots(col ="red", size = 0.04)
```

## Kindergarten

```{r}
tmap_mode("plot")
tm_shape(mpsz_sf) +
  tm_polygons(alpha = 0.3) +
tm_shape(kindergartens_sf) +
  tm_dots(col = "red", size = 0.04)
```

## Library

```{r}
tmap_mode("plot")
tm_shape(mpsz_sf) +
  tm_polygons(alpha = 0.3) +
tm_shape(library_sf) +
  tm_dots(col = "blue", size = 0.04)
```

## Parks

```{r}
tmap_mode("plot")
tm_shape(mpsz_sf) +
  tm_polygons(alpha = 0.3) +
tm_shape(parks_sf) +
  tm_dots(col = "green", size = 0.04)
```

## Bus stop

```{r}
tmap_mode("plot")
tm_shape(mpsz_sf) +
  tm_polygons(alpha = 0.3) +
tm_shape(busstop_sf) +
  tm_dots(col = "blue", size = 0.04)
```

## Good primary school

```{r}
tmap_mode("plot")
tm_shape(mpsz_sf) +
  tm_polygons(alpha = 0.3) +
tm_shape(goodprisch_sf) +
  tm_dots(col = "red", size = 0.04)
```

## Mrt

```{r}
tmap_mode("plot")
tm_shape(mpsz_sf) +
  tm_polygons(alpha = 0.3) +
tm_shape(mrt_sf) +
  tm_dots(col = "blue", size = 0.04)
```

## Primary school

```{r}
tmap_mode("plot")
tm_shape(mpsz_sf) +
  tm_polygons(alpha = 0.3) +
tm_shape(prisch_sf) +
  tm_dots(col = "red", size = 0.04)
```

## Shopping mall

```{r}
tmap_mode("plot")
tm_shape(mpsz_sf) +
  tm_polygons(alpha = 0.3) +
tm_shape(shoppingmall_sf) +
  tm_dots(col = "blue", size = 0.04)
```

## Supermarket

```{r}
tmap_mode("plot")
tm_shape(mpsz_sf) +
  tm_polygons(alpha = 0.3) +
tm_shape(supermarket_sf) +
  tm_dots(col = "blue", size = 0.04)
```
:::

# 4 Locational factors

With aspatial and geospatial data in set, we will now be setting locational factors which will enable us to view the proximity from one location to the area of interest.

## 4.1 CBD location

We will be taking reference from this [website](https://www.latlong.net/place/downtown-core-singapore-20616.html), to get the latitude and longitude of downtown core. SImilarly, we will convert it into the correct CRS, which is 3414.

```{r}
lat <- 1.287953
lng <- 103.851784

cbd_sf <- data.frame(lat, lng) %>%
  st_as_sf(coords = c("lng", "lat"), crs = 4326) %>%
  st_transform(crs = 3414)
```

## 4.2 Converting resale data into sf

Notice that our resale data, resale_sub do not have any CRS.

```{r}
st_crs(resale_sub)
```

In that case, we will be assigning CRS using st_as_sf to convert it into a sf object, while inserting CRS 3414 using st_transform.

```{r}
resale_sub <- st_as_sf(resale_sub, coords = c("LONGITUDE", "LATITUDE"), crs = 4326) %>%
  st_transform(crs = 3414)
```

Let us do a check. Looking good!

```{r}
st_crs(resale_sub)
```

## 4.3 Proximity distance calculation

Earlier, we were just discussing about how locational factors will help us view the proximity from one location to the other. By referencing to our favourite senior, Megan [again](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/?panelset=self-sourced&panelset6=glimpse%28%29&panelset5=recreational%252Flifestyle#data-wrangling-aspatial-data), we will be calculating as such using st_distance() to get the shortest distance, together with rowMins().

```{r}
proximity <- function(df1, df2, varname) {
  dist_matrix <- st_distance(df1, df2) |> 
    drop_units()
  df1[,varname] <- rowMins(dist_matrix)
  return(df1)
}
```

We will then input all the proximity into all of our data, using the newly created proximity() function.

```{r, eval = FALSE}
resale_sub <-
  proximity(resale_sub, cbd_sf, "PROX_CBD") %>%
  proximity(., communityclubs_sf, "PROX_COMMUNITYCLUBS") %>%
  proximity(., childcare_sf, "PROX_CHILDCARE") %>%
  proximity(., kindergartens_sf, "PROX_KINDERGARTEN") %>%
  proximity(., eldercare_sf, "PROX_ELDERCARE") %>%
  proximity(., hawkercentre_sf, "PROX_HAWKER") %>%
  proximity(., busstop_sf, "PROX_BUSSTOP") %>%
  proximity(., mrt_sf, "PROX_MRT") %>%
  proximity(., library_sf, "PROX_LIBRARY") %>%
  proximity(., parks_sf, "PROX_PARK") %>%
  proximity(., goodprisch_sf, "PROX_GOODPRISCH") %>%
  proximity(., shoppingmall_sf, "PROX_MALL") %>%
  proximity(., supermarket_sf, "PROX_SPRMKT")
```

## 4.4 Facility count within radius calculation

As the saying goes, size is not everything. Knowing the distance between one point to the other is simply not enough. Hence, we will use st_distance() and rowSums() to find the particular facility within the radius!

```{r}
num_radius <- function(df1, df2, varname, radius) {
  dist_matrix <- st_distance(df1, df2) %>%
    drop_units() %>%
    as.data.frame()
  df1[,varname] <- rowSums(dist_matrix <= radius)
  return(df1)
}
```

For the scope of our assignment, we are required to set:

-   Numbers of kindergartens within 350m

-   Numbers of childcare centres within 350m

-   Numbers of bus stop within 350m

-   Numbers of primary school within 1km

Let us do that then!

```{r, eval = FALSE}
resale_sub <- 
  num_radius(resale_sub, kindergartens_sf, "NUM_KINDERGARTEN", 350) %>%
  num_radius(., childcare_sf, "NUM_CHILDCARE", 350) %>%
  num_radius(., busstop_sf, "NUM_BUSSTOP", 350) %>%
  num_radius(., prisch_sf, "NUM_PRIMARYSCH", 1000)
```

## 4.5 Saving as RDS file

With that, let us save the updated resale_sub as RDS so that we can use it later on!

```{r, eval = FALSE}
saveRDS(resale_sub, "data/final/resale_sub.rds")
```

And assign it back into resale_sub!

```{r}
resale_sub <- read_rds("data/final/resale_sub.rds")
```

Let us glimpse resale_sub now. Looking good!

```{r}
glimpse(resale_sub)
```

# 5 Regressions

As we will be doing prediction models, we will be exploring various regressions!

## 5.1 Visualising correlation matrix

Recall that we have already removed lease_commence_date earlier on to prevent perfect multi-colinearity. However, we should visualise the matrix to ensure that no variables are co-linear. Note that we have dropped resale_price, as that is the dependent variable.

```{r}
resale_corm <- resale_sub %>% 
  st_drop_geometry() %>% 
  select_if(is.numeric) %>% 
  select(-resale_price)
corrplot::corrplot(cor(resale_corm), 
                   diag = FALSE, 
                   order = "AOE",
                   tl.pos = "td", 
                   tl.cex = 0.4, 
                   method = "number", 
                   type = "upper")
```

Oh my! looks like we forgot that we combined remaining_lease_year from remaining_lease_yr. This shows that we cannot be complacent. Lucky for us, we found it out before anything! Let us remove it using select().

```{r}
resale_sub <- resale_sub %>%
  select(-remaining_lease_yr, -remaining_lease_mth)
```

## 5.2 Splitting into training and testing data

For the purpose on this assignment, we will be splitting the data into:

-   Testing data, ranging from January 2023 to February 2023

-   Training data, from January 2021 to December 2022

```{r}
test_data <- resale_sub %>% 
  filter(month >= "2023-01-01" & month <= "2023-02-28")

train_data <- resale_sub %>%
  filter(month >= "2021-01-01" & month <= "2022-12-31")
```

## 5.3 Multi-linear regression (non-spatial)

Let us build a multi-linear regression now. The beauty of building a regression is that we can choose what to add and what to not add, as long as it is justified. We will not be adding flat model, as we narrowed the scope to 3 room flats. Geometry will not be useful as well. remaining_lease will be omitted as well, as we have remaining_lease_year.

::: panel-tabset
# Glimpse test_data

```{r}
glimpse(test_data)
```

# Regression

```{r}
multi_lr <- lm(resale_price~ Story_Ordinal + remaining_lease_year + floor_area_sqm + PROX_CBD + PROX_ELDERCARE + PROX_HAWKER + PROX_PARK + PROX_LIBRARY + PROX_COMMUNITYCLUBS + PROX_BUSSTOP + PROX_MALL + PROX_SPRMKT + PROX_GOODPRISCH + PROX_MRT + NUM_CHILDCARE + NUM_KINDERGARTEN + NUM_PRIMARYSCH + month, data = train_data)
```

# Summary

```{r}
summary(multi_lr)
```

# Saving as RDS

```{r, eval = FALSE}
write_rds(multi_lr, "data/regmodels/multi_lr.rds")
```

# Reading RDS

```{r}
multi_reg <- read_rds("data/regmodels/multi_lr.rds")
multi_reg
```
:::

## 5.4 GWR predictive method

We will convert sf into sp, so that it can be useful in predicting resale prices of 3 room HDB flats. We will convert using as_Spatial()

```{r}
train_data_sp <- as_Spatial(train_data)
train_data_sp
```

## 5.5 Computing adaptive bandwidth

Similarly, after computing the bandwidth, we will use write_rds() and read_rds for efficiency!

::: panel-tabset
# Adaptive bandwidth

```{r, eval = FALSE}
bw_adaptive <- bw.gwr(resale_price~ Story_Ordinal + remaining_lease_year + floor_area_sqm + PROX_CBD + PROX_ELDERCARE + PROX_HAWKER + PROX_PARK + PROX_LIBRARY + PROX_COMMUNITYCLUBS + PROX_BUSSTOP + PROX_MALL + PROX_SPRMKT + PROX_GOODPRISCH + PROX_MRT + NUM_CHILDCARE + NUM_KINDERGARTEN + NUM_PRIMARYSCH + month, data  = train_data_sp,
                  approach = "CV",
                  kernel = "gaussian",
                  adaptive = TRUE,
                  longlat = FALSE)
```

# Write RDS

```{r, eval = FALSE}
write_rds(bw_adaptive, "data/regmodels/bw_adaptive.rds")
```

# Read RDS

```{r}
bw_adaptive <- read_rds("data/regmodels/bw_adaptive.rds")
```
:::

## **Constructing adaptive bandwidth GWR**

Similarly to above, we will use write_rds() to save, and read_rds() to read the adaptive gwr

::: panel-tabset
# Adaptive GWR

```{r, eval = FALSE}
gwr_adaptive <- gwr.basic(resale_price~ Story_Ordinal + remaining_lease_year + floor_area_sqm + PROX_CBD + PROX_ELDERCARE + PROX_HAWKER + PROX_PARK + PROX_LIBRARY + PROX_COMMUNITYCLUBS + PROX_BUSSTOP + PROX_MALL + PROX_SPRMKT + PROX_GOODPRISCH + PROX_MRT + NUM_CHILDCARE + NUM_KINDERGARTEN + NUM_PRIMARYSCH + month,
                data = train_data_sp,
                          bw = bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive = TRUE,
                          longlat = FALSE)
```

# Write RDS

```{r, eval = FALSE}
write_rds(gwr_adaptive, "data/regmodels/gwr_adaptive.rds")
```

# Read RDS

```{r}
gwr_adaptive <- read_rds("data/regmodels/gwr_adaptive.rds")
```
:::

## 5.6 Extracting Coordinates

With reference to the previous in class exercise, we were taught that we need to use st_coordinates to extract the x & y coordinates of the dataset. We will be using write_rds(), and read_rds() again.

::: panel-tabset
# Code chunk

```{r, eval = FALSE}
coords <- st_coordinates(resale_sub)
coords_training <- st_coordinates(train_data)
coords_testing <- st_coordinates(test_data)
```

# Write RDS

```{r, eval = FALSE}
write_rds(coords_training, "data/regmodels/coords_training.rds" )
write_rds(coords_testing, "data/regmodels/coords_testing.rds")
```

# Read RDS

```{r}
coords_training <- read_rds("data/regmodels/coords_training.rds")
coords_testing <- read_rds("data/regmodels/coords_testing.rds")
```
:::

## 5.7 Dropping geometry field

We will do so by using st_drop_geometry()

::: panel-tabset
# Code chunk

```{r, eval = FALSE}
train_data <- st_drop_geometry(train_data)
```

# Write RDS

```{r, eval = FALSE}
write_rds(train_data, "data/regmodels/train_data.rds")
```

# Read RDS

```{r}
train_data <- read_rds("data/regmodels/train_data.rds")
```
:::

## 5.8 Random forest model

We use set.seed() to ensure that the same value gets generated in our random forest model. Not so random now, is it?

::: panel-tabset
# Code chunk

```{r, eval = FALSE}
set.seed(1234) 

forest <- ranger(resale_price~ Story_Ordinal + remaining_lease_year + floor_area_sqm + PROX_CBD + PROX_ELDERCARE + PROX_HAWKER + PROX_PARK + PROX_LIBRARY + PROX_COMMUNITYCLUBS + PROX_BUSSTOP + PROX_MALL + PROX_SPRMKT + PROX_GOODPRISCH + PROX_MRT + NUM_CHILDCARE + NUM_KINDERGARTEN + NUM_PRIMARYSCH + month, data = train_data)
```

# Write RDS

```{r, eval = FALSE}
write_rds(forest, "data/regmodels/forest.rds")
```

# Read RDS

```{r}
forest <- read_rds("data/regmodels/forest.rds")
```

# Results

```{r}
print(forest)
```
:::

## 5.9 Calibrating geographic random forest using GRF

For this assignment, will be using 30 trees!

Adaptive bandwidth

```{r, eval = FALSE}
bw_grf_adaptive <- grf.bw(formula = resale_price~ Story_Ordinal + remaining_lease_year + floor_area_sqm + PROX_CBD + PROX_ELDERCARE + PROX_HAWKER + PROX_PARK + PROX_LIBRARY + PROX_COMMUNITYCLUBS + PROX_BUSSTOP + PROX_MALL + PROX_SPRMKT + PROX_GOODPRISCH + PROX_MRT + NUM_CHILDCARE + NUM_KINDERGARTEN + NUM_PRIMARYSCH + month,
            dataset = train_data, 
            kernel = "adaptive", 
            coords = coords_training, 
            trees = 30)

```

![](images/bandwidth.png)

```{r, eval = FALSE}
write_rds(bw_grf_adaptive, "data/regmodels/bw_grf_adaptive.rds")
```

We will be assigning 638, which is the highest R2 score to bw_grf_adaptive before running our grf

```{r}
bw_grf_adaptive <- 638
```

## 5.10 **Calibrating Geographical Random Forest Model**

We will be using grf() to calibrate a model to predict HDB resale price.

::: panel-tabset
```{r, eval = FALSE}
set.seed(1234)

gwRF_adaptive <- grf(formula = resale_price~ Story_Ordinal + remaining_lease_year + floor_area_sqm + PROX_CBD + PROX_ELDERCARE + PROX_HAWKER + PROX_PARK + PROX_LIBRARY + PROX_COMMUNITYCLUBS + PROX_BUSSTOP + PROX_MALL + PROX_SPRMKT + PROX_GOODPRISCH + PROX_MRT + NUM_CHILDCARE + NUM_KINDERGARTEN + NUM_PRIMARYSCH + month,
                   dframe = train_data,
                   bw = bw_grf_adaptive,
                   kernel = "adaptive",
                   coords = coords_training,
                   ntree = 30)
```

# Write RDS

```{r, eval = FALSE}
write_rds(gwRF_adaptive, "data/regmodels/gwRF_adaptive.rds")
```

# Read RDS

```{r}
gwRF_adaptive <- read_rds("data/regmodels/gwRF_adaptive.rds")
```
:::

Here are the snippets of the output gwRF for better visualisation!

![](images/table%20pic.png){width="623" height="233"}

![](images/global%20summary%201.jpg){width="630" height="302"}

![](images/global%20summary%202.jpg){width="421"}

# 6 Predictions

Since we are done with the training data, this section will deal with testing data for our prediction

## 6.1 Dropping geometry for test data

::: panel-tabset
# Code chunk

```{r, eval = FALSE}
test_data <- cbind(test_data, coords_testing) %>%
  st_drop_geometry()
```

# Write RDS

```{r, eval = FALSE}
write_rds(test_data, "data/regmodels/test_data.rds")
```

# Read RDS

```{r}
test_data <- read_rds("data/regmodels/test_data.rds")
```
:::

## 6.2 Predicting with test data

We will use predict.grf() to predict the resale value by using the test data and gwRF_adaptive model calibrated above!

::: panel-tabset
# Code chunk

```{r, eval = FALSE}
gwRF_pred <- predict.grf(gwRF_adaptive, 
                           test_data, 
                           x.var.name = "X",
                           y.var.name = "Y", 
                           local.w = 1,
                           global.w = 0)
```

# Write RDS

```{r, eval = FALSE}
GRF_pred <- write_rds(gwRF_pred, "data/regmodels/gwRF_pred.rds")
```
:::

## 6.3 **Conversion of predicting output into data frame**

We will convert the output using as.data.frame()

```{r}
GRF_pred <- read_rds("data/regmodels/gwRF_pred.rds")
GRF_pred_df <- as.data.frame(GRF_pred)
```

Next, we will be binding the predicted output into the test data using cbind()

::: panel-tabset
# Cbind

```{r}
test_data_p <- cbind(test_data, GRF_pred_df)
```

# Write RDS

```{r, eval = FALSE}
write_rds(test_data_p, "data/regmodels/test_data_p.rds")
```

# Read RDS

```{r}
test_data_p <- read_rds("data/regmodels/test_data_p.rds")
```
:::

## 6.4 Calculating root mean square error

rsme() will help us measure how far the predicted values are from the observed values in a regression analysis

```{r}
rmse(test_data_p$resale_price, 
     test_data_p$GRF_pred)
```

## 6.5 **Visualising the predicted values**

Let us visualise it on a scatterplot, where we plot actual resale price against predicted resale price

```{r}
ggplot(data = test_data_p,
       aes(x = GRF_pred,
           y = resale_price)) +
  geom_point()
```

# 7 Conclusion

In summary, while both OLS and GWR are regression techniques used to analyze the relationship between variables, GWR is better suited for analyzing spatially varying relationships, while OLS is better suited for analyzing relationships that are constant across space.

## 7.1 OLS vs Geographical weighted methods

The main difference between OLS and GWR is that OLS produces a single set of coefficients that are applicable across the entire data set, while GWR produces a set of coefficients that vary across space, which also means that the strength and direction of the relationship between the dependent variable and independent variables can change depending on the location of the observation, which can be more useful in understanding resale prices of HDB, especially if we are using the model for prediction! Let us prove this below.

::: panel-tabset
# OLS (non spatial)

```{r}
multi_reg_df <- predict(multi_reg, test_data)
multi_reg_test<- cbind(test_data, multi_reg_df)

rmse(multi_reg_test$resale_price, 
     multi_reg_test$multi_reg_df)
```

```{r}
ggplot(data = multi_reg_test,
       aes(x = multi_reg_df,
           y = resale_price)) +
  geom_point() +
  geom_abline(col = "Red")
```

# Geographical Random Forest Model

```{r}
rmse(test_data_p$resale_price, 
     test_data_p$GRF_pred)
```

```{r}
ggplot(data = test_data_p,
       aes(x = GRF_pred,
           y = resale_price)) +
  geom_point() +
  geom_abline(col = "Red")
```
:::

From the scatter plots and the mean square error between both models, we can tell that the OSL aims to minimise residual sum of squares, as the regression line is fitted in that manner. Geographical Random Forest Model on the other hand, aims to reduce mean squared error, which is further supported by the value as computed above.

Hence, we therefore conclude that the Geographical Random Forest Model is better as a basis of predictive model, as compared to Non-spatial OLS.

## 7.2 Possible Improvements

-   If we recall, we ignored the variable flat_type as we assumed it to be 3 room throughout. In real life scenarios, that is not the case as different flat types do in fact have different default layout, which may affect the resale prices of the house. This could be done by potentially creating a binary variable with each of the different variables in flat type.

-   It was also obvious that there were outliers on the scatterplot. These outliers are the real life scenarios of HDB resale prices appearing out in the news where people are selling and buying houses for 1 million dollars. But hey, what can I say? if there is a demand, there will definitely be a supply. Removing those outlier can certainly help the overall model to improve.

-   Lastly, with a stronger device, we could potentially create more trees generated in the gwRF model!

# 8 References

I am super grateful for all of the resources that were made available to me, especially from past seniors and prof kam's in-class resources.

A huge thanks to our favourite senior, [Megan](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/), for her work which helped many of us during the initial data wrangling and outsourcing stage!

I also referred to another senior's work, [Aisyah](https://aisyahajit2018-is415.netlify.app/posts/2021-11-07-take-home-exercise-3/), where i found inspiration for creating the remaining_lease_year column!

And of course, most importantly to [Prof Kam's](https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex09/in-class_ex09_gwml#calibrating-random-forest-model) in-class exercise 9 for the second half of this take home assignment!
